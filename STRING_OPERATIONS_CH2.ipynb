{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "023639b1",
   "metadata": {},
   "source": [
    "## Bag of Words Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9f73fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2fc72e",
   "metadata": {},
   "source": [
    "### let's define the docs array that contains the paths of doc1.txt, doc2.txt, and doc3.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935e2770",
   "metadata": {},
   "source": [
    "### We need to create a BoW from a list of documents. The documents (doc1.txt, doc2.txt, and doc3.txt) can be found in the your-code directory of this exercise. You will read the content of each document into an array of strings named corpus.\n",
    "### Now let's define the docs array that contains the paths of doc1.txt, doc2.txt, and doc3.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4885669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['doc1.txt', 'doc3.txt', 'doc2.txt']\n"
     ]
    }
   ],
   "source": [
    "docs=[]\n",
    "for filename in os.listdir('./'):\n",
    "    if filename.endswith('.txt'):\n",
    "        docs.append(filename)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebfd0b7",
   "metadata": {},
   "source": [
    "### Define an empty array corpus that will contain the content strings of the docs. Loop docs and read the content of each doc into the corpus array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09e0dee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ironhack is cool.', 'I am a student at Ironhack.', 'I love Ironhack.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "corpus=[]\n",
    "\n",
    "for Filename in os.listdir('./'):\n",
    "    if Filename.endswith('.txt'):\n",
    "        with open(Filename,'r') as f:\n",
    "            content=f.readline()\n",
    "        corpus.append(content)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea2e21c",
   "metadata": {},
   "source": [
    "### Print corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "806a189f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ironhack is cool.', 'I am a student at Ironhack.', 'I love Ironhack.']\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf5fb7",
   "metadata": {},
   "source": [
    "### Write your code below to process corpus (convert to lower case and remove special characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3a387552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ironhack is cool', 'i am a student at ironhack', 'i love ironhack']\n"
     ]
    }
   ],
   "source": [
    "corpus_clean=[(lambda i: i.lower().strip('\\.')) (i) for i in corpus] \n",
    "print(corpus_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b7680e",
   "metadata": {},
   "source": [
    "### Now define bag_of_words as an empty array. It will be used to store the unique terms in corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "db25d016",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347d9002",
   "metadata": {},
   "source": [
    "### Loop through corpus. In each loop, do the following: \n",
    "### 1. Break the string into an array of terms. \n",
    "### 2. Create a sub-loop to iterate the terms array. In each sub-loop, you'll check if the current term is already contained in bag_of_words. If not in bag_of_words, append it to the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e394f4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ironhack', 'is', 'cool', 'i', 'am', 'a', 'student', 'at', 'love']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i in corpus_clean:\n",
    "    terms = i.split(' ')\n",
    "    for word in terms:\n",
    "        if word not in bag_of_words:\n",
    "            bag_of_words.append(word)\n",
    "            \n",
    "            \n",
    "print(bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68664758",
   "metadata": {},
   "source": [
    "### Now we define an empty array called term_freq. Loop corpus for a second time. In each loop, create a sub-loop to iterate the terms in bag_of_words. \n",
    "### Count how many times each term appears in each doc of corpus. Append the term-frequency array to term_freq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "15f5dc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 1, 0, 1, 0, 0, 0], [1, 0, 0, 1, 1, 1, 1, 1, 0], [1, 0, 0, 1, 0, 1, 0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "term_freq=[]\n",
    "\n",
    "count=0\n",
    "    \n",
    "for i in corpus_clean:\n",
    "    tf=[] #porqué hay una diferencia si declaro dentro de aquí?\n",
    "    count=0 \n",
    "    for j in bag_of_words:\n",
    "        if j in i:\n",
    "            tf.append(count+1) #como hacerle si quiero que count se vaya aumentando pero quiero que regrese a 0 despues de cada iteración de i\n",
    "        else:\n",
    "            tf.append(0)\n",
    "    \n",
    "    term_freq.append(tf)\n",
    "\n",
    "print(term_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "5b1f09fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ironhack', 'is', 'cool'], ['i', 'am', 'a', 'student', 'at', 'ironhack'], ['i', 'love', 'ironhack']]\n",
      "[[1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 0, 0, 1, 1, 1, 1, 1, 0], [1, 0, 0, 1, 0, 0, 0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "term_freq2=[]\n",
    "corpus_split=[]\n",
    "\n",
    "for i in corpus_clean:\n",
    "    corpus_split.append(i.split(' '))\n",
    "print(corpus_split)\n",
    "\n",
    "for x in corpus_split:\n",
    "    tf2=[]\n",
    "    \n",
    "    for y in bag_of_words:\n",
    "        #count=0 \n",
    "        if y in x:\n",
    "            tf2.append(1)\n",
    "        else:\n",
    "            tf2.append(0)\n",
    "    term_freq2.append(tf2)\n",
    "            \n",
    "print(term_freq2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "67c81623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       " [1, 0, 0, 1, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 0, 0, 1, 1, 0, 0, 0, 0], [1, 0, 0, 1, 0, 1, 1, 1, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a7e699",
   "metadata": {},
   "source": [
    "## Bonus Question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701c3b09",
   "metadata": {},
   "source": [
    "### Optimize your solution for the above question by removing stop words from the BoW. For your convenience, a list of stop words is defined for you in the next cell.\n",
    "\n",
    "### Requirements:\n",
    "\n",
    "### Combine all your previous codes to the cell below. Improve your solution by ignoring stop words in bag_of_words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7382f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['all', 'six', 'less', 'being', 'indeed', 'over', 'move', 'anyway', 'fifty', 'four', 'not', 'own', 'through', 'yourselves', 'go', 'where', 'mill', 'only', 'find', 'before', 'one', 'whose', 'system', 'how', 'somewhere', 'with', 'thick', 'show', 'had', 'enough', 'should', 'to', 'must', 'whom', 'seeming', 'under', 'ours', 'has', 'might', 'thereafter', 'latterly', 'do', 'them', 'his', 'around', 'than', 'get', 'very', 'de', 'none', 'cannot', 'every', 'whether', 'they', 'front', 'during', 'thus', 'now', 'him', 'nor', 'name', 'several', 'hereafter', 'always', 'who', 'cry', 'whither', 'this', 'someone', 'either', 'each', 'become', 'thereupon', 'sometime', 'side', 'two', 'therein', 'twelve', 'because', 'often', 'ten', 'our', 'eg', 'some', 'back', 'up', 'namely', 'towards', 'are', 'further', 'beyond', 'ourselves', 'yet', 'out', 'even', 'will', 'what', 'still', 'for', 'bottom', 'mine', 'since', 'please', 'forty', 'per', 'its', 'everything', 'behind', 'un', 'above', 'between', 'it', 'neither', 'seemed', 'ever', 'across', 'she', 'somehow', 'be', 'we', 'full', 'never', 'sixty', 'however', 'here', 'otherwise', 'were', 'whereupon', 'nowhere', 'although', 'found', 'alone', 're', 'along', 'fifteen', 'by', 'both', 'about', 'last', 'would', 'anything', 'via', 'many', 'could', 'thence', 'put', 'against', 'keep', 'etc', 'amount', 'became', 'ltd', 'hence', 'onto', 'or', 'con', 'among', 'already', 'co', 'afterwards', 'formerly', 'within', 'seems', 'into', 'others', 'while', 'whatever', 'except', 'down', 'hers', 'everyone', 'done', 'least', 'another', 'whoever', 'moreover', 'couldnt', 'throughout', 'anyhow', 'yourself', 'three', 'from', 'her', 'few', 'together', 'top', 'there', 'due', 'been', 'next', 'anyone', 'eleven', 'much', 'call', 'therefore', 'interest', 'then', 'thru', 'themselves', 'hundred', 'was', 'sincere', 'empty', 'more', 'himself', 'elsewhere', 'mostly', 'on', 'fire', 'am', 'becoming', 'hereby', 'amongst', 'else', 'part', 'everywhere', 'too', 'herself', 'former', 'those', 'he', 'me', 'myself', 'made', 'twenty', 'these', 'bill', 'cant', 'us', 'until', 'besides', 'nevertheless', 'below', 'anywhere', 'nine', 'can', 'of', 'your', 'toward', 'my', 'something', 'and', 'whereafter', 'whenever', 'give', 'almost', 'wherever', 'is', 'describe', 'beforehand', 'herein', 'an', 'as', 'itself', 'at', 'have', 'in', 'seem', 'whence', 'ie', 'any', 'fill', 'again', 'hasnt', 'inc', 'thereby', 'thin', 'no', 'perhaps', 'latter', 'meanwhile', 'when', 'detail', 'same', 'wherein', 'beside', 'also', 'that', 'other', 'take', 'which', 'becomes', 'you', 'if', 'nobody', 'see', 'though', 'may', 'after', 'upon', 'most', 'hereupon', 'eight', 'but', 'serious', 'nothing', 'such', 'why', 'a', 'off', 'whereby', 'third', 'i', 'whole', 'noone', 'sometimes', 'well', 'amoungst', 'yours', 'their', 'rather', 'without', 'so', 'five', 'the', 'first', 'whereas', 'once']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "57216d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ironhack', 'cool', 'student', 'love']\n"
     ]
    }
   ],
   "source": [
    "new_bow=[i for i in bag_of_words if i not in stop_words]\n",
    "print(new_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "89b45613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 0, 0], [1, 0, 1, 0], [1, 0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "term_freq3=[]\n",
    "corpus_split=[]\n",
    "\n",
    "for i in corpus_clean:\n",
    "    corpus_split.append(i.split(' '))\n",
    "\n",
    "for x in corpus_split:\n",
    "    new_tf2=[]\n",
    "    \n",
    "    for y in new_bow:\n",
    "        #count=0 \n",
    "        if y in x:\n",
    "            new_tf2.append(1)\n",
    "        else:\n",
    "            new_tf2.append(0)\n",
    "    term_freq3.append(new_tf2)\n",
    "            \n",
    "print(term_freq3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fcf7a1",
   "metadata": {},
   "source": [
    "## Additional Challenge for the Nerds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0871426b",
   "metadata": {},
   "source": [
    "We will learn Scikit-Learn in Module 3 which has built in the BoW feature. Try to use Scikit-Learn to generate the BoW for this challenge and check whether the output is the same as yours. You will need to do some googling to find out how to use Scikit-Learn to generate BoW.\n",
    "\n",
    "Notes:\n",
    "\n",
    "To install Scikit-Learn, use pip install sklearn.\n",
    "\n",
    "Scikit-Learn removes stop words by default. You don't need to manually remove stop words.\n",
    "\n",
    "Scikit-Learn's output has slightly different format from the output example demonstrated above. It's ok, you don't need to convert the Scikit-Learn output.\n",
    "\n",
    "The Scikit-Learn output will look like below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b30f7232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: scikit-learn in /Users/derinkivaner/opt/anaconda3/lib/python3.9/site-packages (from sklearn) (1.0.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/derinkivaner/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /Users/derinkivaner/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Users/derinkivaner/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/derinkivaner/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->sklearn) (2.2.0)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=b740daf18f82afd450470efe779f1efd725170a846515910c5e70a531ad104a0\n",
      "  Stored in directory: /Users/derinkivaner/Library/Caches/pip/wheels/e4/7b/98/b6466d71b8d738a0c547008b9eb39bf8676d1ff6ca4b22af1c\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "407fb763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am' 'at' 'cool' 'ironhack' 'is' 'love' 'student']\n",
      "{'ironhack': 3, 'is': 4, 'cool': 2, 'am': 0, 'student': 6, 'at': 1, 'love': 5}\n",
      "[[0 0 1 1 1 0 0]\n",
      " [1 1 0 1 0 0 1]\n",
      " [0 0 0 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "#Fit the bag-of-words model\n",
    "NEW_BOW= vectorizer.fit_transform(corpus_clean)\n",
    "# Get unique words/tokens found in akk the documents. The unique words/tokens represents\n",
    "#the features\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "#Associate the inides with each unique word\n",
    "print(vectorizer.vocabulary_)\n",
    "# Print the numerical value vector\n",
    "print(NEW_BOW.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ca4e365b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ironhack is cool', 'i am a student at ironhack', 'i love ironhack']"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab000e72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
